# -*- coding: utf-8 -*-
"""
AISatyagrah Jobs API — FastAPI single-file implementation

Features:
- Auth: x-auth static token and optional JWT (Authorization: Bearer).
- Exporters: CSV, PDF (reportlab), PPTX (python-pptx), GIF (Pillow), MP4 (ffmpeg if available), ZIP of all.
- /api/export/{csv|pdf|pptx|gif|mp4|zip|all}
- Files: /api/files?limit=&offset= with totals; /api/files/cleanup?older_than_days=60
- Jobs (memory + Redis/RQ):
  - POST /api/jobs   -> enqueues memory or Redis job with retries/backoff
  - GET  /api/jobs/{id}
  - DELETE /api/jobs/{id}
  - GET  /api/jobs/{id}/events  (SSE progress stream)
- History: /api/history (SQLite-backed)
- Health/config/version/redis pings
- Prometheus metrics at /metrics
- Minimal /ui and / scripts/windows endpoint for scheduled task scaffolding
"""

import os, io, sys, csv, json, time, math, uuid, glob, shutil, random, string, sqlite3, subprocess, datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
from threading import Thread, Event
from dataclasses import dataclass, field

from fastapi import FastAPI, Request, Response, Depends, Query
from fastapi.responses import JSONResponse, PlainTextResponse, HTMLResponse, StreamingResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware

# Optional deps
try:
    from reportlab.lib.pagesizes import A4
    from reportlab.pdfgen import canvas as rl_canvas
except Exception:
    rl_canvas = None

try:
    from pptx import Presentation
    from pptx.util import Inches, Pt
except Exception:
    Presentation = None

try:
    from PIL import Image, ImageDraw, ImageFont
except Exception:
    Image = None

# Redis/RQ optional
try:
    from redis import Redis  # type: ignore
    from rq import Queue, Retry  # type: ignore
    from rq.job import Job  # type: ignore
except Exception:
    Redis = None
    Queue = None
    Retry = None
    Job = None

VERSION = "0.0.5"

# --------------------------------------------------------------------------------------
# Paths & helpers
# --------------------------------------------------------------------------------------

ROOT = Path(os.environ.get("AISATYAGRAH_ROOT") or Path(__file__).resolve().parents[2])
EXPORTS = ROOT / "exports"
DATA_DIR = ROOT / "data"
DB_PATH = DATA_DIR / "jobs.db"
for p in (EXPORTS, DATA_DIR):
    p.mkdir(parents=True, exist_ok=True)

def now_ts() -> float:
    return time.time()

def today_str() -> str:
    return datetime.date.today().isoformat()

def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def rand_id(prefix: str) -> str:
    return f"{prefix}_{''.join(random.choices(string.ascii_lowercase+string.digits, k=10))}"

def external_base(req: Request) -> str:
    # honor X-External-Base if provided (for reverse proxies), else build from request
    b = req.headers.get("x-external-base")
    if b:
        return b.rstrip("/")
    url = str(req.base_url).rstrip("/")
    return url

def safe_str(o: Any) -> str:
    try:
        return str(o)
    except Exception:
        return repr(o)

def list_export_files(limit: int, offset: int) -> Tuple[List[Dict[str, Any]], int]:
    # Collect all files under exports
    files = []
    for path in EXPORTS.rglob("*"):
        if path.is_file():
            try:
                files.append({
                    "path": str(path),
                    "size": path.stat().st_size,
                    "mtime": path.stat().st_mtime,
                })
            except Exception:
                pass
    files.sort(key=lambda x: x["mtime"], reverse=True)
    total = len(files)
    return files[offset: offset + limit], total

def cleanup_old_exports(older_than_days: int) -> int:
    cutoff = time.time() - older_than_days * 86400
    removed_dirs = 0
    for day_dir in EXPORTS.iterdir():
        if not day_dir.is_dir():
            continue
        # If all files older than cutoff, remove dir
        try:
            newest = 0.0
            for f in day_dir.rglob("*"):
                if f.is_file():
                    newest = max(newest, f.stat().st_mtime)
            if newest and newest < cutoff:
                shutil.rmtree(day_dir, ignore_errors=True)
                removed_dirs += 1
        except Exception:
            pass
    return removed_dirs

def which(cmd: str) -> Optional[str]:
    from shutil import which as _which
    return _which(cmd)

# --------------------------------------------------------------------------------------
# Authentication (x-auth + optional JWT)
# --------------------------------------------------------------------------------------

AUTH_TOKEN = os.environ.get("AUTH_TOKEN", "").strip()
JWT_SECRET = os.environ.get("JWT_SECRET", "").strip()

def _jwt_decode(token: str) -> Optional[Dict[str, Any]]:
    if not JWT_SECRET:
        return None
    try:
        # lightweight JWT check without dependencies (HS256 only)
        import base64, hmac, hashlib
        parts = token.split(".")
        if len(parts) != 3:
            return None
        header_b, payload_b, sig_b = parts
        def b64url_decode(s):
            s += "=" * ((4 - len(s) % 4) % 4)
            return base64.urlsafe_b64decode(s.encode())
        header = json.loads(b64url_decode(header_b))
        payload = json.loads(b64url_decode(payload_b))
        sig = b64url_decode(sig_b)
        signing_input = (header_b + "." + payload_b).encode()
        expected = hmac.new(JWT_SECRET.encode(), signing_input, hashlib.sha256).digest()
        if not hmac.compare_digest(sig, expected):
            return None
        # exp check
        if "exp" in payload and float(payload["exp"]) < time.time():
            return None
        return payload
    except Exception:
        return None

def auth_guard(req: Request) -> None:
    # Allow local GET on "/" and /metrics without auth
    path = req.url.path
    if path in ("/", "/metrics", "/favicon.ico", "/robots.txt"):
        return
    # If no auth configured, allow
    if not AUTH_TOKEN and not JWT_SECRET:
        return
    # Accept x-auth header or ?x-auth=
    tok = req.headers.get("x-auth") or req.query_params.get("x-auth")
    if tok and AUTH_TOKEN and tok == AUTH_TOKEN:
        return
    # Try JWT
    if JWT_SECRET:
        authz = req.headers.get("Authorization", "")
        if authz.startswith("Bearer "):
            jwt = authz.split(" ", 1)[1].strip()
            if _jwt_decode(jwt):
                return
    # Dev override
    if req.query_params.get("no-auth") == "1":
        return
    raise PermissionError("unauthorized")

def auth_dependency(req: Request):
    try:
        auth_guard(req)
    except PermissionError as e:
        raise e

# --------------------------------------------------------------------------------------
# SQLite history store
# --------------------------------------------------------------------------------------

class JobsStore:
    def __init__(self, path: Path):
        self.path = path
        self._init_db()

    def _init_db(self):
        self.path.parent.mkdir(exist_ok=True, parents=True)
        with sqlite3.connect(self.path) as c:
            c.execute("""
            CREATE TABLE IF NOT EXISTS jobs (
              id TEXT PRIMARY KEY,
              backend TEXT,
              kind TEXT,
              status TEXT,
              created_at REAL,
              updated_at REAL,
              result_json TEXT
            )""")
            c.commit()

    def upsert(self, rec: Dict[str, Any]):
        with sqlite3.connect(self.path) as c:
            c.execute("""
            INSERT INTO jobs (id, backend, kind, status, created_at, updated_at, result_json)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(id) DO UPDATE SET
              backend=excluded.backend,
              kind=excluded.kind,
              status=excluded.status,
              created_at=excluded.created_at,
              updated_at=excluded.updated_at,
              result_json=excluded.result_json
            """, (
                rec.get("id"),
                rec.get("backend"),
                rec.get("kind"),
                rec.get("status"),
                rec.get("created_at"),
                rec.get("updated_at"),
                json.dumps(rec.get("result") or {}),
            ))
            c.commit()

    def list(self, limit: int, offset: int) -> Tuple[List[Dict[str, Any]], int]:
        with sqlite3.connect(self.path) as c:
            cur = c.cursor()
            cur.execute("SELECT COUNT(1) FROM jobs")
            total = cur.fetchone()[0]
            cur.execute("SELECT id, backend, kind, status, created_at, updated_at, result_json "
                        "FROM jobs ORDER BY updated_at DESC LIMIT ? OFFSET ?", (limit, offset))
            rows = cur.fetchall()
        items = []
        for (id_, backend, kind, status, created_at, updated_at, resj) in rows:
            try:
                res = json.loads(resj or "{}")
            except Exception:
                res = {}
            items.append({
                "id": id_,
                "backend": backend,
                "kind": kind,
                "status": status,
                "created_at": created_at,
                "updated_at": updated_at,
                "result": res,
            })
        return items, total

HISTORY = JobsStore(DB_PATH)

# --------------------------------------------------------------------------------------
# In-memory job runner with retries/backoff and SSE progress
# --------------------------------------------------------------------------------------

@dataclass
class MemJob:
    id: str
    backend: str = "memory"
    kind: str = "all"
    date: Optional[str] = None
    status: str = "queued"     # queued|running|done|failed|canceled
    progress: float = 0.0
    message: str = ""
    result: Dict[str, Any] = field(default_factory=dict)
    created_at: float = field(default_factory=now_ts)
    updated_at: float = field(default_factory=now_ts)
    attempts: List[Dict[str, Any]] = field(default_factory=list)
    max_retries: int = 0
    backoff_sec: int = 5
    cancel_ev: Event = field(default_factory=Event)

MEM_JOBS: Dict[str, MemJob] = {}

def _mem_job_status(job_id: str) -> Dict[str, Any]:
    j = MEM_JOBS.get(job_id)
    if not j:
        raise KeyError("not_found")
    return {
        "ok": True,
        "id": j.id,
        "backend": j.backend,
        "kind": j.kind,
        "date": j.date,
        "status": j.status,
        "progress": j.progress,
        "message": j.message,
        "result": j.result,
        "attempts": j.attempts,
        "created_at": j.created_at,
        "updated_at": j.updated_at,
    }

def _mem_job_cancel(job_id: str) -> None:
    j = MEM_JOBS.get(job_id)
    if not j:
        return
    j.cancel_ev.set()
    j.status = "canceled"
    j.updated_at = now_ts()

def _run_mem_job(job: MemJob, base_url: str):
    def set_progress(p: float, msg: str):
        job.progress = max(0.0, min(100.0, p))
        job.message = msg
        job.updated_at = now_ts()

    tries = 0
    last_err = None
    while tries <= job.max_retries:
        if job.cancel_ev.is_set():
            set_progress(job.progress, "canceled")
            job.status = "canceled"
            break
        tries += 1
        attempt = {"try": tries, "ts": now_ts()}
        job.attempts.append(attempt)
        job.status = "running"
        set_progress(5.0, f"attempt {tries}")

        try:
            # simulate phases
            for step, msg in [(20, "collecting"), (40, "exporting"), (60, "bundling"), (80, "finalizing")]:
                if job.cancel_ev.is_set():
                    raise RuntimeError("canceled")
                time.sleep(0.2)
                set_progress(step, msg)

            # perform export
            result = _do_export(kind=job.kind, date=job.date, base_url=base_url)
            job.result = result
            set_progress(100.0, "complete")
            job.status = "done"

            # record history
            HISTORY.upsert({
                "id": job.id,
                "backend": "memory",
                "kind": job.kind,
                "status": job.status,
                "created_at": job.created_at,
                "updated_at": job.updated_at,
                "result": job.result,
            })
            break
        except Exception as ex:
            last_err = ex
            job.status = "failed"
            set_progress(job.progress, f"failed: {safe_str(ex)}")
            if tries <= job.max_retries:
                time.sleep(max(1, job.backoff_sec))
            else:
                # record failure
                HISTORY.upsert({
                    "id": job.id,
                    "backend": "memory",
                    "kind": job.kind,
                    "status": job.status,
                    "created_at": job.created_at,
                    "updated_at": job.updated_at,
                    "result": job.result,
                })
    return

def _mem_job_start(kind: str, date: Optional[str], base_url: str, retries: int, backoff_sec: int) -> str:
    jid = rand_id("mem")
    job = MemJob(id=jid, kind=kind, date=date, max_retries=max(0, retries), backoff_sec=max(1, backoff_sec))
    MEM_JOBS[jid] = job
    t = Thread(target=_run_mem_job, args=(job, base_url), daemon=True)
    t.start()
    return jid

# --------------------------------------------------------------------------------------
# Exporters
# --------------------------------------------------------------------------------------

def _collect_demo_rows() -> List[Dict[str, Any]]:
    # Sample rows; replace with real data later
    rows = []
    for i in range(1, 11):
        rows.append({
            "id": i,
            "title": f"Item {i}",
            "url": f"https://example.com/item/{i}",
            "size": random.randint(1000, 50000),
        })
    return rows

def _export_dir_for(date_str: Optional[str]) -> Path:
    day = date_str or today_str()
    dest = EXPORTS / day
    ensure_dir(dest)
    return dest

def export_csv(date: Optional[str]) -> str:
    dest = _export_dir_for(date)
    out = dest / f"export_{date or today_str()}_{time.strftime('%Y-%m-%d_%H%M%S')}.csv"
    rows = _collect_demo_rows()
    with open(out, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))
        w.writeheader()
        for r in rows:
            w.writerow(r)
    return str(out)

def export_pdf(date: Optional[str]) -> str:
    dest = _export_dir_for(date)
    out = dest / f"export_{date or today_str()}_{time.strftime('%Y-%m-%d_%H%M%S')}.pdf"
    if rl_canvas is None:
        # create tiny placeholder if reportlab missing
        with open(out, "wb") as f:
            f.write(b"%PDF-1.4\n% placeholder\n")
        return str(out)
    c = rl_canvas.Canvas(str(out), pagesize=A4)
    width, height = A4
    rows = _collect_demo_rows()
    y = height - 50
    c.setFont("Helvetica-Bold", 14)
    c.drawString(40, y, "AISatyagrah Export")
    y -= 20
    c.setFont("Helvetica", 10)
    for r in rows:
        txt = f"[{r['id']}] {r['title']}  size={r['size']}   {r['url']}"
        c.drawString(40, y, txt)
        y -= 14
        if y < 60:
            c.showPage()
            y = height - 50
    c.showPage()
    c.save()
    return str(out)

def export_pptx(date: Optional[str]) -> str:
    dest = _export_dir_for(date)
    out = dest / f"export_{date or today_str()}_{time.strftime('%Y-%m-%d_%H%M%S')}.pptx"
    if Presentation is None:
        # placeholder file
        with open(out, "wb") as f:
            f.write(b"PPTX placeholder")
        return str(out)
    prs = Presentation()
    title_slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(title_slide_layout)
    slide.shapes.title.text = "AISatyagrah Export"
    slide.placeholders[1].text = f"Generated {time.strftime('%Y-%m-%d %H:%M:%S')}"

    rows = _collect_demo_rows()
    bullet_layout = prs.slide_layouts[1]
    slide = prs.slides.add_slide(bullet_layout)
    slide.shapes.title.text = "Items"
    body = slide.shapes.placeholders[1].text_frame
    for r in rows[:10]:
        body.add_paragraph().text = f"#{r['id']} {r['title']} ({r['size']} bytes)"

    prs.save(out)
    return str(out)

def export_gif(date: Optional[str]) -> str:
    dest = _export_dir_for(date)
    out = dest / f"export_{date or today_str()}_{time.strftime('%Y-%m-%d_%H%M%S')}.gif"
    if Image is None:
        with open(out, "wb") as f:
            f.write(b"GIF89a")  # minimal header placeholder
        return str(out)
    frames = []
    for i in range(6):
        img = Image.new("RGB", (480, 270), (10 + i*20, 40, 80 + i*25))
        d = ImageDraw.Draw(img)
        d.text((10, 10), f"AISatyagrah {i+1}/6", fill=(255, 255, 255))
        frames.append(img)
    frames[0].save(out, save_all=True, append_images=frames[1:], duration=200, loop=0)
    return str(out)

def export_mp4(date: Optional[str]) -> str:
    dest = _export_dir_for(date)
    out = dest / f"export_{date or today_str()}_{time.strftime('%Y-%m-%d_%H%M%S')}.mp4"
    # Prefer ffmpeg if available
    ff = which("ffmpeg")
    if ff:
        tmp = dest / f"tmp_{uuid.uuid4().hex}.gif"
        try:
            gif_path = export_gif(date)
            # create mp4 from gif
            subprocess.run([ff, "-y", "-f", "gif", "-i", gif_path, "-movflags", "faststart",
                            "-pix_fmt", "yuv420p", str(out)], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except Exception:
            with open(out, "wb") as f:
                f.write(b"mp4 placeholder")
        finally:
            try:
                if tmp.exists():
                    tmp.unlink()
            except Exception:
                pass
    else:
        # fallback placeholder
        with open(out, "wb") as f:
            f.write(b"mp4 placeholder")
    return str(out)

def export_zip(date: Optional[str], artifacts: Dict[str, str]) -> str:
    dest = _export_dir_for(date)
    out = dest / f"export_{date or today_str()}_{time.strftime('%Y-%m-%d_%H%M%S')}.zip"
    import zipfile
    with zipfile.ZipFile(out, "w", compression=zipfile.ZIP_DEFLATED) as z:
        for k, p in artifacts.items():
            try:
                z.write(p, arcname=Path(p).name)
            except Exception:
                pass
    return str(out)

def _do_export(kind: str, date: Optional[str], base_url: str) -> Dict[str, str]:
    # returns dict of artifacts paths
    res: Dict[str, str] = {}
    if kind in ("csv", "all"):
        res["csv"] = export_csv(date)
    if kind in ("pdf", "all"):
        res["pdf"] = export_pdf(date)
    if kind in ("pptx", "all"):
        res["pptx"] = export_pptx(date)
    if kind in ("gif", "all"):
        res["gif"] = export_gif(date)
    if kind in ("mp4", "all"):
        res["mp4"] = export_mp4(date)
    if kind in ("zip", "all"):
        # if zip alone was requested, ensure we have something to bundle
        if "csv" not in res and "pdf" not in res and "pptx" not in res and "gif" not in res and "mp4" not in res:
            # create a minimal CSV as content
            res["csv"] = export_csv(date)
        res["zip"] = export_zip(date, res)
    return res

# --------------------------------------------------------------------------------------
# RQ / Redis helpers
# --------------------------------------------------------------------------------------

def _redis_enabled() -> bool:
    return bool(Redis and Queue and os.environ.get("REDIS_URL"))

def _get_rq_queue():
    if not _redis_enabled():
        raise RuntimeError("redis/rq not available")
    url = os.environ.get("REDIS_URL", "redis://127.0.0.1:6379/0")
    conn = Redis.from_url(url)
    return Queue("exports", connection=conn)

def rq_run_export(kind: str, date: Optional[str], base_url: str) -> Dict[str, str]:
    # This function must be top-level (picklable). RQ will import it by module path.
    result = _do_export(kind=kind, date=date, base_url=base_url)
    # We can’t know the RQ job id here directly unless we read current_job; skip history for RQ or store ad-hoc
    return result

# --------------------------------------------------------------------------------------
# Metrics (Prometheus)
# --------------------------------------------------------------------------------------

METRICS = {
    "exports_started_total": 0,
    "exports_succeeded_total": 0,
    "exports_failed_total": 0,
    "export_duration_seconds_sum": 0.0,
    "export_duration_seconds_count": 0,
}

def _metrics_text(queue_len_redis: int, queue_len_mem: int) -> str:
    return (
        "# HELP exports_started_total Number of exports started\n"
        "# TYPE exports_started_total counter\n"
        f"exports_started_total {METRICS['exports_started_total']}\n"
        "# HELP exports_succeeded_total Number of exports succeeded\n"
        "# TYPE exports_succeeded_total counter\n"
        f"exports_succeeded_total {METRICS['exports_succeeded_total']}\n"
        "# HELP exports_failed_total Number of exports failed\n"
        "# TYPE exports_failed_total counter\n"
        f"exports_failed_total {METRICS['exports_failed_total']}\n"
        "# HELP export_duration_seconds_sum Sum of durations\n"
        "# TYPE export_duration_seconds_sum counter\n"
        f"export_duration_seconds_sum {METRICS['export_duration_seconds_sum']}\n"
        "# HELP export_duration_seconds_count Count of durations\n"
        "# TYPE export_duration_seconds_count counter\n"
        f"export_duration_seconds_count {METRICS['export_duration_seconds_count']}\n"
        "# HELP export_queue_length jobs pending in queue (redis+memory)\n"
        "# TYPE export_queue_length gauge\n"
        f"export_queue_length{{backend='redis'}} {queue_len_redis}\n"
        f"export_queue_length{{backend='memory'}} {queue_len_mem}\n"
    )

# --------------------------------------------------------------------------------------
# FastAPI app
# --------------------------------------------------------------------------------------

def create_app() -> FastAPI:
    app = FastAPI(title="AISatyagrah Jobs API", version=VERSION)

    # CORS
    origins = [
        "http://127.0.0.1:8010",
        "http://localhost:8010",
        "http://127.0.0.1:9000",
        "http://localhost:9000",
    ]
    app.add_middleware(
        CORSMiddleware,
        allow_origins=origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # ---------------- Root & Utility ----------------

    @app.get("/", response_class=HTMLResponse)
    async def root():
        return f"""
        <html><head><title>AISatyagrah Jobs API</title></head>
        <body>
          <h2>AISatyagrah Jobs API</h2>
          <ul>
            <li>GET <code>/api/version</code></li>
            <li>GET <code>/api/health</code></li>
            <li>GET <code>/api/config</code></li>
            <li>POST <code>/api/export/all</code> etc.</li>
            <li>POST <code>/api/jobs</code>, GET/DELETE <code>/api/jobs/&lt;id&gt;</code>, SSE <code>/api/jobs/&lt;id&gt;/events</code></li>
            <li>GET <code>/api/files</code>, POST <code>/api/files/cleanup</code></li>
            <li>GET <code>/api/history</code></li>
            <li>GET <code>/metrics</code> (Prometheus)</li>
          </ul>
        </body></html>"""

    @app.get("/api/health")
    async def api_health(req: Request, _=Depends(auth_dependency)):
        return {"ok": True, "ts": now_ts()}

    @app.get("/api/version")
    async def api_version(req: Request, _=Depends(auth_dependency)):
        return {"ok": True, "version": VERSION}

    @app.get("/api/config")
    async def api_config(req: Request, _=Depends(auth_dependency)):
        return {
            "ok": True,
            "root": str(ROOT),
            "auth_enabled": bool(AUTH_TOKEN or JWT_SECRET),
            "jwt_enabled": bool(JWT_SECRET),
            "ui_origins": [
                "http://127.0.0.1:9000",
                "http://localhost:9000",
                "http://127.0.0.1:8010",
                "http://localhost:8010",
            ],
            "redis_enabled": _redis_enabled(),
            "redis_url_set": bool(os.environ.get("REDIS_URL")),
            "db_backend": "sqlite",
            "debug": bool(os.environ.get("DEBUG")),
        }

    @app.get("/api/redis")
    async def api_redis(req: Request, _=Depends(auth_dependency)):
        if not _redis_enabled():
            return {"ok": True, "enabled": False}
        try:
            q = _get_rq_queue()
            q.connection.ping()
            return {"ok": True, "enabled": True}
        except Exception:
            return {"ok": False, "enabled": True, "error": "ping_failed"}

    # ---------------- Export endpoints ----------------

    async def _export(kind: str, req: Request):
        base = external_base(req)
        data = {}
        try:
            if req.method in ("POST", "PUT", "PATCH"):
                data = await req.json()
        except Exception:
            data = {}
        date = data.get("date")
        t0 = time.time()
        METRICS["exports_started_total"] += 1
        try:
            result = _do_export(kind=kind, date=date, base_url=base)
            dt = time.time() - t0
            METRICS["exports_succeeded_total"] += 1
            METRICS["export_duration_seconds_sum"] += dt
            METRICS["export_duration_seconds_count"] += 1
            # record synthetic “instant” job in history
            HISTORY.upsert({
                "id": rand_id("exp"),
                "backend": "direct",
                "kind": kind,
                "status": "done",
                "created_at": now_ts(),
                "updated_at": now_ts(),
                "result": result,
            })
            return {"ok": True, "date": date or today_str(), **result}
        except Exception as ex:
            METRICS["exports_failed_total"] += 1
            return JSONResponse({"ok": False, "error": "export_failed", "detail": str(ex)}, status_code=500)

    @app.post("/api/export/all")
    async def api_export_all(req: Request, _=Depends(auth_dependency)):
        return await _export("all", req)

    @app.post("/api/export/csv")
    async def api_export_csv(req: Request, _=Depends(auth_dependency)):
        return await _export("csv", req)

    @app.post("/api/export/pdf")
    async def api_export_pdf(req: Request, _=Depends(auth_dependency)):
        return await _export("pdf", req)

    @app.post("/api/export/pptx")
    async def api_export_pptx(req: Request, _=Depends(auth_dependency)):
        return await _export("pptx", req)

    @app.post("/api/export/gif")
    async def api_export_gif(req: Request, _=Depends(auth_dependency)):
        return await _export("gif", req)

    @app.post("/api/export/mp4")
    async def api_export_mp4(req: Request, _=Depends(auth_dependency)):
        return await _export("mp4", req)

    @app.post("/api/export/zip")
    async def api_export_zip(req: Request, _=Depends(auth_dependency)):
        return await _export("zip", req)

    # ---------------- Files & cleanup ----------------

    @app.get("/api/files")
    async def api_files(req: Request, limit: int = 10, offset: int = 0, _=Depends(auth_dependency)):
        limit = max(1, min(100, int(limit)))
        offset = max(0, int(offset))
        items, total = list_export_files(limit, offset)
        return {"ok": True, "items": items, "total": total, "limit": limit, "offset": offset}

    @app.post("/api/files/cleanup")
    async def api_files_cleanup(req: Request, older_than_days: int = Query(60, ge=1, le=3650), _=Depends(auth_dependency)):
        removed = cleanup_old_exports(older_than_days)
        return {"ok": True, "removed_dirs": removed, "older_than_days": older_than_days}

    # ---------------- Jobs API (Memory + Redis) ----------------

    @app.post("/api/jobs")
    async def api_jobs_start(req: Request, _=Depends(auth_dependency)):
        data = {}
        try:
            data = await req.json()
        except Exception:
            pass
        kind = str(data.get("kind") or "all")
        date = data.get("date")
        use_redis = bool(data.get("use_redis", False))
        retries = int(data.get("retries", 0))
        backoff_sec = int(data.get("backoff_sec", 5))
        base = external_base(req)

        if use_redis:
            if not _redis_enabled():
                return JSONResponse({"ok": False, "error": "redis_disabled"}, status_code=400)
            try:
                q = _get_rq_queue()
                retry_obj = Retry(max=retries, interval=backoff_sec) if (retries and Retry) else None
                job = q.enqueue(rq_run_export, kind, date, base, job_timeout=60*60, retry=retry_obj)
                return {"ok": True, "backend": "redis", "job_id": job.id}
            except Exception as ex:
                return JSONResponse({"ok": False, "error": "enqueue_failed", "detail": str(ex)}, status_code=500)

        # memory job
        jid = _mem_job_start(kind=kind, date=date, base_url=base, retries=retries, backoff_sec=backoff_sec)
        return {"ok": True, "backend": "memory", "job_id": jid}

    @app.get("/api/jobs/{job_id}")
    async def api_jobs_get(job_id: str, req: Request, _=Depends(auth_dependency)):
        # try memory
        if job_id in MEM_JOBS:
            return _mem_job_status(job_id)
        # try redis
        if _redis_enabled() and Job:
            try:
                q = _get_rq_queue()
                job = Job.fetch(job_id, connection=q.connection)
                st = job.get_status()
                res = None
                try:
                    res = job.result
                except Exception:
                    res = None
                return {
                    "ok": True,
                    "id": job_id,
                    "backend": "redis",
                    "kind": "all",
                    "status": st,
                    "created_at": None,
                    "updated_at": None,
                    "result": res or {},
                }
            except Exception:
                return JSONResponse({"ok": False, "error": "not_found"}, status_code=404)
        return JSONResponse({"ok": False, "error": "not_found"}, status_code=404)

    @app.delete("/api/jobs/{job_id}")
    async def api_jobs_delete(job_id: str, req: Request, _=Depends(auth_dependency)):
        # cancel memory
        if job_id in MEM_JOBS:
            _mem_job_cancel(job_id)
            return {"ok": True}
        # cancel redis
        if _redis_enabled() and Job:
            try:
                q = _get_rq_queue()
                job = Job.fetch(job_id, connection=q.connection)
                job.cancel()
                return {"ok": True}
            except Exception as ex:
                return JSONResponse({"ok": False, "error": "cancel_failed", "detail": str(ex)}, status_code=500)
        return JSONResponse({"ok": False, "error": "not_found"}, status_code=404)

    @app.get("/api/jobs/{job_id}/events")
    async def api_jobs_events(job_id: str, req: Request):
        # SSE stream. Auth is optional here to simplify UI dev; enforce if you like:
        try:
            auth_guard(req)
        except Exception:
            # allow unauth? comment next line to require auth
            return StreamingResponse(_sse_error("unauthorized"), media_type="text/event-stream")

        async def event_gen():
            # Poll and stream progress
            for _ in range(0, 2400):  # ~20 minutes @0.5s
                # memory job
                if job_id in MEM_JOBS:
                    j = MEM_JOBS[job_id]
                    payload = {
                        "ok": True,
                        "id": j.id,
                        "status": j.status,
                        "progress": j.progress,
                        "message": j.message,
                        "result": j.result,
                        "updated_at": j.updated_at,
                    }
                    yield "event: progress\n"
                    yield f"data: {json.dumps(payload)}\n\n"
                    if j.status in ("done", "failed", "canceled"):
                        break
                    await _sleep(0.5)
                    continue
                # redis job
                if _redis_enabled() and Job:
                    try:
                        q = _get_rq_queue()
                        job = Job.fetch(job_id, connection=q.connection)
                        st = job.get_status()
                        res = None
                        try:
                            res = job.result
                        except Exception:
                            res = None
                        payload = {"ok": True, "id": job_id, "status": st, "result": res or {}}
                        yield "event: progress\n"
                        yield f"data: {json.dumps(payload)}\n\n"
                        if st in ("finished", "failed", "canceled", "stopped"):
                            break
                        await _sleep(0.5)
                        continue
                    except Exception:
                        pass
                # not found yet
                yield "event: progress\n"
                yield f"data: {json.dumps({'ok': False, 'error': 'not_found'})}\n\n"
                await _sleep(0.5)

        return StreamingResponse(event_gen(), media_type="text/event-stream")

    # ---------------- History (SQLite) ----------------

    @app.get("/api/history")
    async def api_history(req: Request, limit: int = 20, offset: int = 0, _=Depends(auth_dependency)):
        limit = max(1, min(100, int(limit)))
        offset = max(0, int(offset))
        items, total = HISTORY.list(limit=limit, offset=offset)
        return {"ok": True, "items": items, "total": total, "limit": limit, "offset": offset}

    # ---------------- Metrics ----------------

    @app.get("/metrics")
    async def metrics(req: Request):
        # queue lengths
        mem_len = len([j for j in MEM_JOBS.values() if j.status in ("queued", "running")])
        red_len = 0
        if _redis_enabled():
            try:
                q = _get_rq_queue()
                red_len = q.count
            except Exception:
                pass
        return PlainTextResponse(_metrics_text(red_len, mem_len), media_type="text/plain; version=0.0.4")

    # ---------------- Scripts (Windows scheduled task scaffolding) ----------------

    @app.get("/api/scripts/windows")
    async def scripts_windows(req: Request, kind: str = "scheduled_task", _=Depends(auth_dependency)):
        if kind != "scheduled_task":
            return JSONResponse({"ok": False, "error": "unknown_kind"}, status_code=400)
        ps = f"""# Create Scheduled Tasks for AISatyagrah API and RQ worker
$root = "{ROOT}".Replace("\\","/")
$py   = "$root/.venv/Scripts/python.exe"
$api  = "uvicorn satyagrah.web.jobs_api:create_app --factory --host 127.0.0.1 --port 9000"
$wrkr = "python -m rq.cli worker exports --url $env:REDIS_URL"

$actionApi  = New-ScheduledTaskAction -Execute $py -Argument "-m uvicorn satyagrah.web.jobs_api:create_app --factory --host 127.0.0.1 --port 9000"
$actionWrk  = New-ScheduledTaskAction -Execute $py -Argument "-m rq.cli worker exports --url $env:REDIS_URL"
$trigger    = New-ScheduledTaskTrigger -AtStartup
Register-ScheduledTask -TaskName "AISatyagrah-API"    -Action $actionApi -Trigger $trigger -RunLevel Highest -Force
Register-ScheduledTask -TaskName "AISatyagrah-Worker" -Action $actionWrk -Trigger $trigger -RunLevel Highest -Force
"""
        return PlainTextResponse(ps, media_type="text/plain; charset=utf-8")

    return app

# Async tiny sleeper for SSE
async def _sleep(sec: float):
    import asyncio
    await asyncio.sleep(sec)

def _sse_error(msg: str):
    yield "event: error\n"
    yield f"data: {json.dumps({'ok': False, 'error': msg})}\n\n"

# Convenience for "uvicorn satyagrah.web.jobs_api:app"
app = create_app()
